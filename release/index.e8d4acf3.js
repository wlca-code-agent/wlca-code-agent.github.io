let e,t;const i=document.getElementById("header"),a=new Map;let n=!1;const s=e=>{window.scrollTo({top:e.offsetTop-i.offsetHeight,behavior:"smooth"})},o=e=>{r(e)&&s(e)},r=e=>e.getBoundingClientRect().top-i.offsetHeight<0||e.getBoundingClientRect().top>window.innerHeight,c=()=>window.matchMedia("(min-width: 1576px)").matches;document.querySelectorAll(".nav-bar ul li").forEach(e=>{let t=e.querySelector("a")?.getAttribute("href")?.substring(1);if(!t)return;let i=document.getElementById(t);i&&(a.set(i,e),e.addEventListener("click",t=>{t.preventDefault(),l(e),n=!0,s(i),setTimeout(()=>{n=!1},800)}))});const l=t=>{t!=e&&(e?.classList.remove("active"),t?.classList.add("active"),e=t)},d=e=>{let t;let s=window.scrollY,o=Math.max(0,Math.min(1,(s-50)/1));if(i.style.backgroundColor=`rgba(0, 0, 0, ${o})`,e&&(n=!1),n)return;let r=!0,c=s+window.innerHeight;for(let[e,n]of a.entries()){let a=e.offsetTop-i.offsetHeight,o=a+e.offsetHeight;if(r&&s<=a-50||s<=o+2&&c>=a-2&&(t=n,c>=o-2))break;r&&(r=!1)}l(t)};window.addEventListener("scroll",()=>{d()}),d();const h=document.querySelector(".talk-content"),g={title:".talk-title-text > div:first-child",speaker:".speaker-text > div:nth-child(2)",abstract:".talk-abstract-text > div:nth-child(2)",bio:".speaker-bio-text > div:nth-child(2)"},u={};for(const e in g){let t=g[e];u[e]=h.querySelector(`${t}`)}const f=[{title:"Evaluating and Harnessing Large Language Models for Automated Penetration Testing",speaker:"Tianwei Zhang",abstract:"Penetration testing, a crucial industrial practice for ensuring system security, has traditionally resisted automation due to the extensive expertise required by human professionals. Large Language Models (LLMs) have shown significant advancements in various domains, and their emergent abilities suggest their potential to revolutionize industries. In this work, we establish a comprehensive benchmark using real-world penetration testing targets and further use it to explore the capabilities of LLMs in this domain. Our findings reveal that while LLMs demonstrate proficiency in specific sub-tasks within the penetration testing process, such as using testing tools, interpreting outputs, and proposing subsequent actions, they also encounter difficulties maintaining a whole context of the overall testing scenario. Based on these insights, we introduce PENTESTGPT, an LLM-empowered automated penetration testing framework that leverages the abundant domain knowledge inherent in LLMs. PENTESTGPT is meticulously designed with three self-interacting modules, each addressing individual sub-tasks of penetration testing, to mitigate the challenges related to context loss. It not only outperforms LLMs among the benchmark targets, but also proves effective in tackling real-world penetration testing targets and CTF challenges. Having been open-sourced on GitHub, PENTESTGPT has garnered over 6,500 stars in 12 months and fostered active community engagement, attesting to its value and impact in both the academic and industrial spheres.",bio:"Dr. Tianwei Zhang is currently an associate professor at College of Computing and Data Science, Nanyang Technological University, Singapore. He received his Bachelor's degree at Peking University in 2011, and Ph.D degree at Princeton University in 2017. His research focuses on building efficient and trustworthy computer systems. He has been involved in the organization committee of numerous technical conferences, including serving as the general chair of KSEM'22. He serves on the editorial board of IEEE Transactions on Circuits and Systems for Video Technology (TCSVT) since 2021, and receives the best editor award in 2023. He has published more than 150 papers in top-tier security, AI, and system conferences and journals. He has received several research awards, including Distinguished Paper Award @ ASPLOS'23, Distinguished Paper Award @ ACL'24, Distinguished Artifact Award @ Usenix Security'24, Distinguished Artifact Award @ CCS'24. "},{title:"Automated Generation of Formal Program Specifications via Large Language Models",speaker:"Yi Li",abstract:"In the software development process, formal program specifications play a crucial role in various stages, including requirement analysis, software testing, and verification. However, manually crafting program specifications is rather difficult, making the job time-consuming and labour-intensive. To tackle this challenge, we introduce SpecGen, a novel technique for formal program specification generation based on Large Language Models (LLMs). Our key insight is to overcome the limitations of existing methods by leveraging the code comprehension capability of LLMs. The process of SpecGen consists of two phases. The first phase employs a conversational approach that guides the LLM to generate appropriate specifications for a given program, aiming to utilize the ability of LLM to generate high-quality specifications. The second phase, designed for where the LLM fails to generate correct specifications, applies four mutation operators to the model-generated specifications and selects verifiable specifications from the mutated ones through a novel heuristic selection strategy by assigning different weights of variants in an efficient manner. We evaluate SpecGen on two datasets, including the SV-COMP Java category benchmark and a manually constructed dataset containing 120 programs. Experimental results demonstrate that SpecGen succeeds in generating verifiable specifications for 279 out of 385 programs, outperforming the existing purely LLM-based approaches and conventional specification generation tools like Houdini and Daikon. Further investigations on the quality of generated specifications indicate that SpecGen can comprehensively articulate the behaviors of the input program.",bio:"Yi Li is an Associate Professor at the College of Computing and Data Science, Nanyang Technological University (NTU) and an Associate Director of the NTU Centre in Computational Technologies for Finance (CCTF). Dr Li has been leading the Software Reliability and Security Lab (SRSLab@NTU) since 2018. His research interests are in program analysis and automated reasoning techniques with applications in software engineering and software security. Together with his research team, he develops solutions enabling the construction of high-quality software systems that are both reliable and sustainable. Currently, his work focuses on the security and fairness of decentralized applications and blockchain systems, as well as the robustness and dependability of AI systems. His work in these areas won four ACM Distinguished Paper Awards and two Best Artifact Awards at top-tier conferences, including ASE'15, ICSME'20, FSE'21, ISSTA'22, and ASE'23."},{title:"Large Language Models for Automated Program Repair",speaker:"Xiaofei Xie",abstract:"Large Language Models (LLMs) have shown exceptional achievements in diverse applications, particular in code generation tasks. In this talk, I will introduce a LLM-based Automated Program Repair (APR) approach that leverages conversation-driven mechanisms augmented by contrastive test pairs. This technique primarily aims to minimize discrepancies between generated passing tests and associated failing tests. Such a strategy enhances the ability of LLMs to isolate the root causes of bugs effectively. Moreover, while LLMs have achieved significant achievements, the reliance on extensive training corpora raises concerns about whether these successes stem from genuine comprehension or merely from data memorization. To address this, we further introduce a hypothesis testing method specifically designed to dissect the memorization phenomena within APR contexts. Our preliminary findings suggest that many fixes generated by LLMs may indeed be attributed to memorization, highlighting the critical need for more robust and rigorous evaluations in LLM-based tasks.",bio:"Dr. Xiaofei Xie is an Assistant Professor and  Lee Kong Chian Fellow at Singapore Management University. He obtained his Ph.D from Tianjin University and won the CCF Outstanding Doctoral Dissertation Award (2019) in China. Previously, he was a Wallenberg-NTU Presidential Postdoctoral Fellow at NTU. His research mainly focuses on the quality assurance of both traditional software and AI-enabled software. He has published top-tier conference/journal papers in the areas of software engineering, security and AI. In particular, he has received four ACM SIGSOFT Distinguished Paper Awards (FSE'16, ASE '19, ISSTA '22 and ASE '23) and a APSEC Best Paper Award."},{title:"Generating Domain-Specific Tests via LLM-based Conversation with Code Repository",speaker:"Binhang Qi",abstract:`Software testing has been a long-standing challenge in the software engineering community. While automatic test generators like EvoSuite (focused on branch coverage) and LLM-based solutions (focused on code generation) show promising results, they often fall short when it comes to incorporating project-specific domain knowledge. This gap makes the generated tests less practical and harder to pass code review or be integrated into a software product.
In this talk, I will share our work DTester, an LLM-based test generator designed to generate practical project-specific tests. First, our empirical study reveals that sophisticated software projects contain abundant code assets valuable for guiding the generation of new tests. Based on the empirical observation, DTester is designed to retrieve a referable test and adapt it to the target test through LLM-based conversation with the project, thus capturing crucial domain-specific information. We also implement DTester as a VSCode extension to facilitate the practical use of test generation.'`,bio:"Binhang Qi is currently a Postdoctoral Researcher at School of Computing, National University of Singapore, working with Professors Jin-Song Dong and Yun Lin. He received PhD degree from School of Computer Science and Engineering at Beihang University (BUAA) in 2024. His main research interests include DNN modularization and test generation, with his work published in ASE, ICSE, FSE, and TOSEM. He received the ACM SIGSOFT Distinguished Paper Award at ICSE 2024."},{title:"CoEdPilot: Recommending Code Edits with Learned Prior Edit Relevance, Project-wise Awareness, and Interactive Nature",speaker:"Chenyan Liu",abstract:"Incremental code edits are more frequent than generating new code in software projects. To automate this process, existing language model-based solutions focus primarily on generating edit solutions based on given location and relevant prior edits. However, editing tasks can be more complicated: It is non-trivial to infer the subsequent edit location, as the scope of edit ripple effect can be the whole project. Moreover, editing sessions may contain multiple (ir)relevant edits. In this talk, I will share our work CoEdPilot, a LLM-driven framework for code evolution assistance. CoEdPilot orchestrates a set of neural Transformers, for discriminating relevant edits, monitoring the ripple effects of edits, exploring their interactive natures and generating edit solution. We also implement CoEdPilot as a VS Code extension for user-friendly interaction.",bio:`Chenyan Liu is currently a PhD student at School of Computing, National University of Singapore. He is supervised by Dr. Yun LIN,\xa0Dr. Jin Song DONG\xa0and\xa0Dr. ZhiYong HUANG. He received his Bachelor's degree at Huazhong University of Science and Technology in 2021 and Master's degree at National University of Singapore in 2023. His research focuses on the design and evaluation of code evolution systems.`}],p=async e=>{let t=new Promise(t=>{setTimeout(t,e)});await t},m=document.querySelector("#speakers-section .section-title"),v=document.querySelector(".talk-content-wrapper");document.querySelectorAll(".talk-abstract-text")[0];const w=async(e,i)=>{if(i&&(c()?s(m):o(v)),e==t)return;t=e,h.classList.add("fading"),await p(200);let a=f[e];if(a)for(let e in u)u[e].innerText=a[e];h.classList.remove("fading");let n=L.get(e);S.forEach(e=>{e.classList.remove("selected")}),n.classList.add("selected")};document.querySelectorAll(".speaker-name > a").forEach(e=>{let t=e.querySelector(".material-symbols-outlined");e.addEventListener("mouseover",()=>{t.classList.add("active")}),e.addEventListener("mouseleave",()=>{t.classList.remove("active")})});const y=new Map;f.forEach((e,t)=>{y.set(e.speaker,t)});const b=e=>e.querySelector(".speaker-name > a").childNodes[0].textContent.trim(),L=new Map,S=document.querySelectorAll(".speaker-card");S.forEach(e=>{let t=b(e),i=y.get(t);void 0!==i&&(L.set(i,e),e.addEventListener("mouseover",()=>{e.classList.add("active")}),e.addEventListener("mouseleave",()=>{e.classList.remove("active")}),e.addEventListener("click",()=>{w(i,!0)}))}),w(0),document.querySelectorAll(".schedule-table tbody tr").forEach((e,t)=>{let i=e.querySelector("td:nth-child(2)"),a=e.querySelector("td:last-child");i.textContent=f[t].speaker,a.textContent=f[t].title});