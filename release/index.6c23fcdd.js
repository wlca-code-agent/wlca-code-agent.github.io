let e,t;const i=document.getElementById("header"),a=new Map;let n=!1;const o=e=>{window.scrollTo({top:e.offsetTop-i.offsetHeight,behavior:"smooth"})};document.querySelectorAll(".nav-bar ul li").forEach(e=>{let t=e.querySelector("a")?.getAttribute("href")?.substring(1);if(!t)return;let i=document.getElementById(t);i&&(a.set(i,e),e.addEventListener("click",t=>{t.preventDefault(),s(e),n=!0,o(i),setTimeout(()=>{n=!1},800)}))});const s=t=>{t!=e&&(e?.classList.remove("active"),t?.classList.add("active"),e=t)},r=e=>{let t;let o=window.scrollY,r=Math.max(0,Math.min(1,(o-0)/1));if(i.style.backgroundColor=`rgba(0, 0, 0, ${r})`,e&&(n=!1),n)return;let c=!0,l=o+window.innerHeight;for(let[e,n]of a.entries()){let a=e.offsetTop-i.offsetHeight,s=a+e.offsetHeight;if(c&&o<=a-50||o<=s+2&&l>=a-2&&(t=n,l>=s-2))break;c&&(c=!1)}s(t)};window.addEventListener("scroll",()=>{r()}),r();const c=document.querySelector(".talk-content"),l={title:".talk-title-text > div:first-child",speaker:".speaker-text > div:nth-child(2)",abstract:".talk-abstract-text > div:nth-child(2)",bio:".speaker-bio-text > div:nth-child(2)"},d={};for(const e in l){let t=l[e];d[e]=c.querySelector(`${t}`)}const h=[{title:"Large Language Models for Automated Program Repair",speaker:"Xiaofei Xie",abstract:"Large Language Models (LLMs) have shown exceptional achievements in diverse applications, particular in code generation tasks. In this talk, I will introduce a LLM-based Automated Program Repair (APR) approach that leverages conversation-driven mechanisms augmented by contrastive test pairs. This technique primarily aims to minimize discrepancies between generated passing tests and associated failing tests. Such a strategy enhances the ability of LLMs to isolate the root causes of bugs effectively. Moreover, while LLMs have achieved significant achievements, the reliance on extensive training corpora raises concerns about whether these successes stem from genuine comprehension or merely from data memorization. To address this, we further introduce a hypothesis testing method specifically designed to dissect the memorization phenomena within APR contexts. Our preliminary findings suggest that many fixes generated by LLMs may indeed be attributed to memorization, highlighting the critical need for more robust and rigorous evaluations in LLM-based tasks.",bio:"Dr. Xiaofei Xie is an Assistant Professor and  Lee Kong Chian Fellow at Singapore Management University. He obtained his Ph.D from Tianjin University and won the CCF Outstanding Doctoral Dissertation Award (2019) in China. Previously, he was a Wallenberg-NTU Presidential Postdoctoral Fellow at NTU. His research mainly focuses on the quality assurance of both traditional software and AI-enabled software. He has published top-tier conference/journal papers in the areas of software engineering, security and AI. In particular, he has received four ACM SIGSOFT Distinguished Paper Awards (FSE'16, ASE '19, ISSTA '22 and ASE '23) and a APSEC Best Paper Award."},{title:"Generating Domain-Specific Tests via LLM-based Conversation with Code Repository",speaker:"Binhang Qi",abstract:`Software testing has been a long-standing challenge in the software engineering community. While automatic test generators like EvoSuite (focused on branch coverage) and LLM-based solutions (focused on code generation) show promising results, they often fall short when it comes to incorporating project-specific domain knowledge. This gap makes the generated tests less practical and harder to pass code review or be integrated into a software product.
In this talk, I will share our work DTester, an LLM-based test generator designed to generate practical project-specific tests. First, our empirical study reveals that sophisticated software projects contain abundant code assets valuable for guiding the generation of new tests. Based on the empirical observation, DTester is designed to retrieve a referable test and adapt it to the target test through LLM-based conversation with the project, thus capturing crucial domain-specific information. We also implement DTester as a VSCode extension to facilitate the practical use of test generation.'`,bio:"Binhang Qi is currently a Postdoctoral Researcher at School of Computing, National University of Singapore, working with Professors Jin-Song Dong and Yun Lin. He received PhD degree from School of Computer Science and Engineering at Beihang University (BUAA) in 2024. His main research interests include DNN modularization and test generation, with his work published in ASE, ICSE, FSE, and TOSEM. He received the ACM SIGSOFT Distinguished Paper Award at ICSE 2024."},{title:"CoEdPilot: Recommending Code Edits with Learned Prior Edit Relevance, Project-wise Awareness, and Interactive Nature",speaker:"Chenyan Liu",abstract:"Incremental code edits are more frequent than generating new code in software projects. To automate this process, existing language model-based solutions focus primarily on generating edit solutions based on given location and relevant prior edits. However, editing tasks can be more complicated: It is non-trivial to infer the subsequent edit location, as the scope of edit ripple effect can be the whole project. Moreover, editing sessions may contain multiple (ir)relevant edits. In this talk, I will share our work CoEdPilot, a LLM-driven framework for code evolution assistance. CoEdPilot orchestrates a set of neural Transformers, for discriminating relevant edits, monitoring the ripple effects of edits, exploring their interactive natures and generating edit solution. We also implement CoEdPilot as a VS Code extension for user-friendly interaction.",bio:"Chenyan Liu is currently a PhD student at School of Computing, National University of Singapore. He is supervised by Dr. Yun LIN, Dr. Jin Song DONG and Dr. ZhiYong HUANG. He received his Bachelor’s degree at Huazhong University of Science and Technology in 2021 and Master’s degree at National University of Singapore in 2023. His research focuses on the design and evaluation of code evolution systems."}],u=async e=>{let t=new Promise(t=>{setTimeout(t,e)});await t},g=document.querySelector(".talk-choices");h.forEach((e,t)=>{let i=document.createElement("li");i.classList.add("inline-round-text"),i.innerHTML=`${t+1}`,g.append(i)});const m=document.querySelectorAll(".talk-choices > li"),f=document.getElementById("talks-section"),p=async(e,i)=>{if(i&&o(f),e==t)return;t=e,m.forEach((t,i)=>{i==e?t.classList.remove("outline"):t.classList.add("outline")}),c.classList.add("fading"),await u(200);let a=h[e];if(a)for(let e in d)d[e].innerText=a[e];c.classList.remove("fading")};m.forEach((e,t)=>{e.addEventListener("click",()=>{p(t,!0)})}),p(0);const v=document.querySelector(".title-text"),w=document.querySelector(".title-desc"),y=()=>{let e=window.scrollY,t=1.1-.1*Math.min(1,e/120),i=Math.min(120,e)/2;v.style.transform=`scale(${t}) translateY(${i}px)`,w.style.transform=`scale(${t}) translateY(${i}px)`};window.addEventListener("scroll",()=>{y()}),document.querySelectorAll(".speaker-name > a").forEach(e=>{let t=e.querySelector(".material-symbols-outlined");e.addEventListener("mouseover",()=>{t.classList.add("active")}),e.addEventListener("mouseleave",()=>{t.classList.remove("active")})});const L=new Map;h.forEach((e,t)=>{L.set(e.speaker,t)});const b=e=>e.querySelector(".speaker-name > a").childNodes[0].textContent.trim();document.querySelectorAll(".speaker-card").forEach(e=>{let t=b(e),i=L.get(t);void 0!==i&&(e.addEventListener("mouseover",()=>{e.classList.add("active")}),e.addEventListener("mouseleave",()=>{e.classList.remove("active")}),e.addEventListener("click",()=>{p(i,!0)}))});